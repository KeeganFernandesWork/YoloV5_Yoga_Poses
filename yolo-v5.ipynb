{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2a5488",
   "metadata": {
    "papermill": {
     "duration": 0.004272,
     "end_time": "2022-08-15T05:52:50.082359",
     "exception": false,
     "start_time": "2022-08-15T05:52:50.078087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Object Detection Using Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae963a",
   "metadata": {
    "papermill": {
     "duration": 0.002941,
     "end_time": "2022-08-15T05:52:50.088642",
     "exception": false,
     "start_time": "2022-08-15T05:52:50.085701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### For more information check my official blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8e8f0",
   "metadata": {
    "papermill": {
     "duration": 0.002848,
     "end_time": "2022-08-15T05:52:50.094588",
     "exception": false,
     "start_time": "2022-08-15T05:52:50.091740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will be cloning repository and all it's requirements. For more information on this check the official [tutorial on training custom data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614ee989",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-15T05:52:50.103258Z",
     "iopub.status.busy": "2022-08-15T05:52:50.102730Z",
     "iopub.status.idle": "2022-08-15T05:53:05.525162Z",
     "shell.execute_reply": "2022-08-15T05:53:05.523949Z"
    },
    "papermill": {
     "duration": 15.430116,
     "end_time": "2022-08-15T05:53:05.528049",
     "exception": false,
     "start_time": "2022-08-15T05:52:50.097933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\r\n",
      "remote: Enumerating objects: 12927, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (309/309), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (147/147), done.\u001b[K\r\n",
      "remote: Total 12927 (delta 194), reused 265 (delta 162), pack-reused 12618\u001b[K\r\n",
      "Receiving objects: 100% (12927/12927), 12.68 MiB | 18.01 MiB/s, done.\r\n",
      "Resolving deltas: 100% (8847/8847), done.\r\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 5)) (3.5.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 6)) (1.21.6)\r\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 7)) (4.5.4.60)\r\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 8)) (9.1.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 9)) (6.0)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 10)) (2.28.1)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 11)) (1.7.3)\r\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 12)) (1.11.0)\r\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 13)) (0.12.0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 14)) (4.64.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 15)) (3.19.4)\r\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 18)) (2.6.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 23)) (1.3.5)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 24)) (0.11.2)\r\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 38)) (7.33.0)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from -r ./yolov5/requirements.txt (line 39)) (5.9.1)\r\n",
      "Collecting thop>=0.1.1\r\n",
      "  Downloading thop-0.1.1.post2207130030-py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->-r ./yolov5/requirements.txt (line 5)) (4.33.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->-r ./yolov5/requirements.txt (line 5)) (1.4.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->-r ./yolov5/requirements.txt (line 5)) (21.3)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->-r ./yolov5/requirements.txt (line 5)) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->-r ./yolov5/requirements.txt (line 5)) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.2->-r ./yolov5/requirements.txt (line 5)) (0.11.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->-r ./yolov5/requirements.txt (line 10)) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->-r ./yolov5/requirements.txt (line 10)) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->-r ./yolov5/requirements.txt (line 10)) (2022.6.15)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->-r ./yolov5/requirements.txt (line 10)) (2.1.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7.0->-r ./yolov5/requirements.txt (line 12)) (4.1.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (3.3.7)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (0.37.1)\r\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (1.35.0)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (2.1.2)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (59.8.0)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (1.8.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (0.6.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (0.4.6)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (1.1.0)\r\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (1.43.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.1.4->-r ./yolov5/requirements.txt (line 23)) (2022.1)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (5.3.0)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (3.0.30)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (0.2.0)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (0.1.3)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (5.1.1)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (0.7.5)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (0.18.1)\r\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (2.12.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->-r ./yolov5/requirements.txt (line 38)) (4.8.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (4.2.4)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (4.8)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (1.16.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (0.2.7)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (1.3.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->-r ./yolov5/requirements.txt (line 38)) (0.8.3)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (4.12.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->-r ./yolov5/requirements.txt (line 38)) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r ./yolov5/requirements.txt (line 38)) (0.2.5)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r ./yolov5/requirements.txt (line 18)) (3.2.0)\r\n",
      "Installing collected packages: thop\r\n",
      "Successfully installed thop-0.1.1.post2207130030\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git \n",
    "!cd ./yolov5\n",
    "!pip install -r ./yolov5/requirements.txt #install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6564db2",
   "metadata": {
    "papermill": {
     "duration": 0.006225,
     "end_time": "2022-08-15T05:53:05.540966",
     "exception": false,
     "start_time": "2022-08-15T05:53:05.534741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3449d6ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-15T05:53:05.555631Z",
     "iopub.status.busy": "2022-08-15T05:53:05.555268Z",
     "iopub.status.idle": "2022-08-15T05:53:06.522173Z",
     "shell.execute_reply": "2022-08-15T05:53:06.520988Z"
    },
    "papermill": {
     "duration": 0.977481,
     "end_time": "2022-08-15T05:53:06.524827",
     "exception": false,
     "start_time": "2022-08-15T05:53:05.547346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── __notebook__.ipynb\r\n",
      "└── \u001b[01;34myolov5\u001b[00m\r\n",
      "    ├── CONTRIBUTING.md\r\n",
      "    ├── LICENSE\r\n",
      "    ├── README.md\r\n",
      "    ├── \u001b[01;34mdata\u001b[00m\r\n",
      "    ├── detect.py\r\n",
      "    ├── export.py\r\n",
      "    ├── hubconf.py\r\n",
      "    ├── \u001b[01;34mmodels\u001b[00m\r\n",
      "    ├── requirements.txt\r\n",
      "    ├── setup.cfg\r\n",
      "    ├── train.py\r\n",
      "    ├── tutorial.ipynb\r\n",
      "    ├── \u001b[01;34mutils\u001b[00m\r\n",
      "    └── val.py\r\n",
      "\r\n",
      "4 directories, 12 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree -L 2\n",
    "# This shows all the files created "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24867617",
   "metadata": {
    "papermill": {
     "duration": 0.00591,
     "end_time": "2022-08-15T05:53:06.537095",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.531185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following is a *yaml* file that shows the location of the various files to use yolos train.py the files and their labels need to be arranged in a specific order and folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629ff642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-15T05:53:06.550656Z",
     "iopub.status.busy": "2022-08-15T05:53:06.550333Z",
     "iopub.status.idle": "2022-08-15T05:53:06.584288Z",
     "shell.execute_reply": "2022-08-15T05:53:06.583412Z"
    },
    "papermill": {
     "duration": 0.043271,
     "end_time": "2022-08-15T05:53:06.586330",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.543059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = {'path': '/kaggle/input/yoga-poses-annotations',\n",
    "         'train': '/kaggle/input/yoga-poses-annotations/Poses',\n",
    "         'val': '/kaggle/input/yoga-poses-annotations/Poses',\n",
    "         'nc': 4,\n",
    "         'names': ['Dogdown','goddess','tree',\"warrior2\"]}\n",
    " \n",
    "with open(\"data.yaml\", \"w\") as file:\n",
    "   yaml.dump(config, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67528f",
   "metadata": {
    "papermill": {
     "duration": 0.005658,
     "end_time": "2022-08-15T05:53:06.598141",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.592483",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will be using the larger yolo model **yolov5x** feel free to change the model type. Apart from this all the default hyperparameters will be used.Please look at the official [Yolo documentation for hyperparameter optimization](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52ddca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-15T05:53:06.610946Z",
     "iopub.status.busy": "2022-08-15T05:53:06.610680Z",
     "iopub.status.idle": "2022-08-15T05:53:06.615377Z",
     "shell.execute_reply": "2022-08-15T05:53:06.614332Z"
    },
    "papermill": {
     "duration": 0.013741,
     "end_time": "2022-08-15T05:53:06.617704",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.603963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIZE = 640\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "MODEL = \"yolov5x\" \n",
    "WORKERS = 1\n",
    "PROJECT = \"Yoga_Poses\"\n",
    "RUN_NAME = f\"{MODEL}_size{SIZE}_epochs{EPOCHS}_batch{BATCH_SIZE}_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e2f253",
   "metadata": {
    "papermill": {
     "duration": 0.005804,
     "end_time": "2022-08-15T05:53:06.629411",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.623607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "207864d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-12T16:49:19.887761Z",
     "iopub.status.busy": "2022-08-12T16:49:19.887362Z",
     "iopub.status.idle": "2022-08-12T16:49:20.868310Z",
     "shell.execute_reply": "2022-08-12T16:49:20.867178Z",
     "shell.execute_reply.started": "2022-08-12T16:49:19.887724Z"
    },
    "papermill": {
     "duration": 0.005626,
     "end_time": "2022-08-15T05:53:06.641412",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.635786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a4dbd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-15T05:53:06.654705Z",
     "iopub.status.busy": "2022-08-15T05:53:06.653870Z",
     "iopub.status.idle": "2022-08-15T05:53:56.033182Z",
     "shell.execute_reply": "2022-08-15T05:53:56.031946Z"
    },
    "papermill": {
     "duration": 49.388507,
     "end_time": "2022-08-15T05:53:56.035677",
     "exception": false,
     "start_time": "2022-08-15T05:53:06.647170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B disabled.\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=./data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=1, project=Yoga_Poses, name=yolov5x_size640_epochs20_batch64_small, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\r\n",
      "YOLOv5 🚀 v6.1-394-gd7bc5d7 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 runs in ClearML\r\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Yoga_Poses', view at http://localhost:6006/\r\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\r\n",
      "100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 14.0MB/s]\r\n",
      "YOLOv5 temporarily requires wandb version 0.12.10 or below. Some features may not work as expected.\r\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5x.pt to yolov5x.pt...\r\n",
      "100%|████████████████████████████████████████| 166M/166M [00:06<00:00, 27.7MB/s]\r\n",
      "\r\n",
      "Overriding model.yaml nc=80 with nc=4\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \r\n",
      "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \r\n",
      "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \r\n",
      "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \r\n",
      "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \r\n",
      "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \r\n",
      "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \r\n",
      "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \r\n",
      "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \r\n",
      "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \r\n",
      " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \r\n",
      " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \r\n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \r\n",
      " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \r\n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n",
      " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \r\n",
      " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \r\n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \r\n",
      " 24      [17, 20, 23]  1     60561  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\r\n",
      "Model summary: 567 layers, 86238001 parameters, 86238001 gradients, 204.7 GFLOPs\r\n",
      "\r\n",
      "Transferred 739/745 items from yolov5x.pt\r\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\r\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\r\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/kaggle/input/yoga-poses-annotations/Poses/labels' images and l\u001b[0m\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Cache directory /kaggle/input/yoga-poses-annotations/Poses is not writeable: [Errno 30] Read-only file system: '/kaggle/input/yoga-poses-annotations/Poses/labels.cache.npy'\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/kaggle/input/yoga-poses-annotations/Poses/labels' images and lab\u001b[0m\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING: Cache directory /kaggle/input/yoga-poses-annotations/Poses is not writeable: [Errno 30] Read-only file system: '/kaggle/input/yoga-poses-annotations/Poses/labels.cache.npy'\r\n",
      "Plotting labels to Yoga_Poses/yolov5x_size640_epochs20_batch64_small/labels.jpg... \r\n",
      "\r\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.46 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 1 dataloader workers\r\n",
      "Logging results to \u001b[1mYoga_Poses/yolov5x_size640_epochs20_batch64_small\u001b[0m\r\n",
      "Starting training for 20 epochs...\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]                                           \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./yolov5/train.py\", line 633, in <module>\r\n",
      "    main(opt)\r\n",
      "  File \"./yolov5/train.py\", line 529, in main\r\n",
      "    train(opt.hyp, opt, device, callbacks)\r\n",
      "  File \"./yolov5/train.py\", line 310, in train\r\n",
      "    pred = model(imgs)  # forward\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/kaggle/working/yolov5/models/yolo.py\", line 136, in forward\r\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n",
      "  File \"/kaggle/working/yolov5/models/yolo.py\", line 159, in _forward_once\r\n",
      "    x = m(x)  # run\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/kaggle/working/yolov5/models/common.py\", line 47, in forward\r\n",
      "    return self.act(self.bn(self.conv(x)))\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\r\n",
      "    return forward_call(*input, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py\", line 394, in forward\r\n",
      "    return F.silu(input, inplace=self.inplace)\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\", line 2031, in silu\r\n",
      "    return torch._C._nn.silu_(input)\r\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 15.90 GiB total capacity; 14.77 GiB already allocated; 87.75 MiB free; 14.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f37dee127a0>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\r\n",
      "AttributeError: 'NoneType' object has no attribute 'python_exit_status'\r\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f37dee127a0>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\r\n",
      "AttributeError: 'NoneType' object has no attribute 'python_exit_status'\r\n"
     ]
    }
   ],
   "source": [
    "!wandb disabled # Since we can't display the results on kaggle we will disable W&B\n",
    "!python ./yolov5/train.py --img {SIZE}\\\n",
    "               --batch {BATCH_SIZE}\\\n",
    "               --epochs {EPOCHS}\\\n",
    "               --data ./data.yaml\\\n",
    "               --weights {MODEL}.pt\\\n",
    "               --workers {WORKERS}\\\n",
    "               --project {PROJECT}\\\n",
    "               --name {RUN_NAME}\\\n",
    "               --exist-ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e90db4c",
   "metadata": {
    "papermill": {
     "duration": 0.011932,
     "end_time": "2022-08-15T05:53:56.060608",
     "exception": false,
     "start_time": "2022-08-15T05:53:56.048676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41eae34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-15T05:53:56.087755Z",
     "iopub.status.busy": "2022-08-15T05:53:56.086684Z",
     "iopub.status.idle": "2022-08-15T06:12:44.185001Z",
     "shell.execute_reply": "2022-08-15T06:12:44.183753Z"
    },
    "papermill": {
     "duration": 1128.115022,
     "end_time": "2022-08-15T06:12:44.187653",
     "exception": false,
     "start_time": "2022-08-15T05:53:56.072631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=./data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=20, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=1, project=Yoga_Poses, name=freeze_layers, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\r\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\r\n",
      "YOLOv5 🚀 v6.1-394-gd7bc5d7 Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\r\n",
      "\r\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\r\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 runs in ClearML\r\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Yoga_Poses', view at http://localhost:6006/\r\n",
      "YOLOv5 temporarily requires wandb version 0.12.10 or below. Some features may not work as expected.\r\n",
      "Overriding model.yaml nc=80 with nc=4\r\n",
      "\r\n",
      "                 from  n    params  module                                  arguments                     \r\n",
      "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \r\n",
      "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \r\n",
      "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \r\n",
      "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \r\n",
      "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \r\n",
      "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \r\n",
      "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \r\n",
      "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \r\n",
      "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \r\n",
      "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \r\n",
      " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \r\n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \r\n",
      " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \r\n",
      " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \r\n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \r\n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \r\n",
      " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \r\n",
      " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \r\n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \r\n",
      " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \r\n",
      " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \r\n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \r\n",
      " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \r\n",
      " 24      [17, 20, 23]  1     60561  models.yolo.Detect                      [4, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\r\n",
      "Model summary: 567 layers, 86238001 parameters, 86238001 gradients, 204.7 GFLOPs\r\n",
      "\r\n",
      "Transferred 739/745 items from yolov5x.pt\r\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\r\n",
      "freezing model.0.conv.weight\r\n",
      "freezing model.0.bn.weight\r\n",
      "freezing model.0.bn.bias\r\n",
      "freezing model.1.conv.weight\r\n",
      "freezing model.1.bn.weight\r\n",
      "freezing model.1.bn.bias\r\n",
      "freezing model.2.cv1.conv.weight\r\n",
      "freezing model.2.cv1.bn.weight\r\n",
      "freezing model.2.cv1.bn.bias\r\n",
      "freezing model.2.cv2.conv.weight\r\n",
      "freezing model.2.cv2.bn.weight\r\n",
      "freezing model.2.cv2.bn.bias\r\n",
      "freezing model.2.cv3.conv.weight\r\n",
      "freezing model.2.cv3.bn.weight\r\n",
      "freezing model.2.cv3.bn.bias\r\n",
      "freezing model.2.m.0.cv1.conv.weight\r\n",
      "freezing model.2.m.0.cv1.bn.weight\r\n",
      "freezing model.2.m.0.cv1.bn.bias\r\n",
      "freezing model.2.m.0.cv2.conv.weight\r\n",
      "freezing model.2.m.0.cv2.bn.weight\r\n",
      "freezing model.2.m.0.cv2.bn.bias\r\n",
      "freezing model.2.m.1.cv1.conv.weight\r\n",
      "freezing model.2.m.1.cv1.bn.weight\r\n",
      "freezing model.2.m.1.cv1.bn.bias\r\n",
      "freezing model.2.m.1.cv2.conv.weight\r\n",
      "freezing model.2.m.1.cv2.bn.weight\r\n",
      "freezing model.2.m.1.cv2.bn.bias\r\n",
      "freezing model.2.m.2.cv1.conv.weight\r\n",
      "freezing model.2.m.2.cv1.bn.weight\r\n",
      "freezing model.2.m.2.cv1.bn.bias\r\n",
      "freezing model.2.m.2.cv2.conv.weight\r\n",
      "freezing model.2.m.2.cv2.bn.weight\r\n",
      "freezing model.2.m.2.cv2.bn.bias\r\n",
      "freezing model.2.m.3.cv1.conv.weight\r\n",
      "freezing model.2.m.3.cv1.bn.weight\r\n",
      "freezing model.2.m.3.cv1.bn.bias\r\n",
      "freezing model.2.m.3.cv2.conv.weight\r\n",
      "freezing model.2.m.3.cv2.bn.weight\r\n",
      "freezing model.2.m.3.cv2.bn.bias\r\n",
      "freezing model.3.conv.weight\r\n",
      "freezing model.3.bn.weight\r\n",
      "freezing model.3.bn.bias\r\n",
      "freezing model.4.cv1.conv.weight\r\n",
      "freezing model.4.cv1.bn.weight\r\n",
      "freezing model.4.cv1.bn.bias\r\n",
      "freezing model.4.cv2.conv.weight\r\n",
      "freezing model.4.cv2.bn.weight\r\n",
      "freezing model.4.cv2.bn.bias\r\n",
      "freezing model.4.cv3.conv.weight\r\n",
      "freezing model.4.cv3.bn.weight\r\n",
      "freezing model.4.cv3.bn.bias\r\n",
      "freezing model.4.m.0.cv1.conv.weight\r\n",
      "freezing model.4.m.0.cv1.bn.weight\r\n",
      "freezing model.4.m.0.cv1.bn.bias\r\n",
      "freezing model.4.m.0.cv2.conv.weight\r\n",
      "freezing model.4.m.0.cv2.bn.weight\r\n",
      "freezing model.4.m.0.cv2.bn.bias\r\n",
      "freezing model.4.m.1.cv1.conv.weight\r\n",
      "freezing model.4.m.1.cv1.bn.weight\r\n",
      "freezing model.4.m.1.cv1.bn.bias\r\n",
      "freezing model.4.m.1.cv2.conv.weight\r\n",
      "freezing model.4.m.1.cv2.bn.weight\r\n",
      "freezing model.4.m.1.cv2.bn.bias\r\n",
      "freezing model.4.m.2.cv1.conv.weight\r\n",
      "freezing model.4.m.2.cv1.bn.weight\r\n",
      "freezing model.4.m.2.cv1.bn.bias\r\n",
      "freezing model.4.m.2.cv2.conv.weight\r\n",
      "freezing model.4.m.2.cv2.bn.weight\r\n",
      "freezing model.4.m.2.cv2.bn.bias\r\n",
      "freezing model.4.m.3.cv1.conv.weight\r\n",
      "freezing model.4.m.3.cv1.bn.weight\r\n",
      "freezing model.4.m.3.cv1.bn.bias\r\n",
      "freezing model.4.m.3.cv2.conv.weight\r\n",
      "freezing model.4.m.3.cv2.bn.weight\r\n",
      "freezing model.4.m.3.cv2.bn.bias\r\n",
      "freezing model.4.m.4.cv1.conv.weight\r\n",
      "freezing model.4.m.4.cv1.bn.weight\r\n",
      "freezing model.4.m.4.cv1.bn.bias\r\n",
      "freezing model.4.m.4.cv2.conv.weight\r\n",
      "freezing model.4.m.4.cv2.bn.weight\r\n",
      "freezing model.4.m.4.cv2.bn.bias\r\n",
      "freezing model.4.m.5.cv1.conv.weight\r\n",
      "freezing model.4.m.5.cv1.bn.weight\r\n",
      "freezing model.4.m.5.cv1.bn.bias\r\n",
      "freezing model.4.m.5.cv2.conv.weight\r\n",
      "freezing model.4.m.5.cv2.bn.weight\r\n",
      "freezing model.4.m.5.cv2.bn.bias\r\n",
      "freezing model.4.m.6.cv1.conv.weight\r\n",
      "freezing model.4.m.6.cv1.bn.weight\r\n",
      "freezing model.4.m.6.cv1.bn.bias\r\n",
      "freezing model.4.m.6.cv2.conv.weight\r\n",
      "freezing model.4.m.6.cv2.bn.weight\r\n",
      "freezing model.4.m.6.cv2.bn.bias\r\n",
      "freezing model.4.m.7.cv1.conv.weight\r\n",
      "freezing model.4.m.7.cv1.bn.weight\r\n",
      "freezing model.4.m.7.cv1.bn.bias\r\n",
      "freezing model.4.m.7.cv2.conv.weight\r\n",
      "freezing model.4.m.7.cv2.bn.weight\r\n",
      "freezing model.4.m.7.cv2.bn.bias\r\n",
      "freezing model.5.conv.weight\r\n",
      "freezing model.5.bn.weight\r\n",
      "freezing model.5.bn.bias\r\n",
      "freezing model.6.cv1.conv.weight\r\n",
      "freezing model.6.cv1.bn.weight\r\n",
      "freezing model.6.cv1.bn.bias\r\n",
      "freezing model.6.cv2.conv.weight\r\n",
      "freezing model.6.cv2.bn.weight\r\n",
      "freezing model.6.cv2.bn.bias\r\n",
      "freezing model.6.cv3.conv.weight\r\n",
      "freezing model.6.cv3.bn.weight\r\n",
      "freezing model.6.cv3.bn.bias\r\n",
      "freezing model.6.m.0.cv1.conv.weight\r\n",
      "freezing model.6.m.0.cv1.bn.weight\r\n",
      "freezing model.6.m.0.cv1.bn.bias\r\n",
      "freezing model.6.m.0.cv2.conv.weight\r\n",
      "freezing model.6.m.0.cv2.bn.weight\r\n",
      "freezing model.6.m.0.cv2.bn.bias\r\n",
      "freezing model.6.m.1.cv1.conv.weight\r\n",
      "freezing model.6.m.1.cv1.bn.weight\r\n",
      "freezing model.6.m.1.cv1.bn.bias\r\n",
      "freezing model.6.m.1.cv2.conv.weight\r\n",
      "freezing model.6.m.1.cv2.bn.weight\r\n",
      "freezing model.6.m.1.cv2.bn.bias\r\n",
      "freezing model.6.m.2.cv1.conv.weight\r\n",
      "freezing model.6.m.2.cv1.bn.weight\r\n",
      "freezing model.6.m.2.cv1.bn.bias\r\n",
      "freezing model.6.m.2.cv2.conv.weight\r\n",
      "freezing model.6.m.2.cv2.bn.weight\r\n",
      "freezing model.6.m.2.cv2.bn.bias\r\n",
      "freezing model.6.m.3.cv1.conv.weight\r\n",
      "freezing model.6.m.3.cv1.bn.weight\r\n",
      "freezing model.6.m.3.cv1.bn.bias\r\n",
      "freezing model.6.m.3.cv2.conv.weight\r\n",
      "freezing model.6.m.3.cv2.bn.weight\r\n",
      "freezing model.6.m.3.cv2.bn.bias\r\n",
      "freezing model.6.m.4.cv1.conv.weight\r\n",
      "freezing model.6.m.4.cv1.bn.weight\r\n",
      "freezing model.6.m.4.cv1.bn.bias\r\n",
      "freezing model.6.m.4.cv2.conv.weight\r\n",
      "freezing model.6.m.4.cv2.bn.weight\r\n",
      "freezing model.6.m.4.cv2.bn.bias\r\n",
      "freezing model.6.m.5.cv1.conv.weight\r\n",
      "freezing model.6.m.5.cv1.bn.weight\r\n",
      "freezing model.6.m.5.cv1.bn.bias\r\n",
      "freezing model.6.m.5.cv2.conv.weight\r\n",
      "freezing model.6.m.5.cv2.bn.weight\r\n",
      "freezing model.6.m.5.cv2.bn.bias\r\n",
      "freezing model.6.m.6.cv1.conv.weight\r\n",
      "freezing model.6.m.6.cv1.bn.weight\r\n",
      "freezing model.6.m.6.cv1.bn.bias\r\n",
      "freezing model.6.m.6.cv2.conv.weight\r\n",
      "freezing model.6.m.6.cv2.bn.weight\r\n",
      "freezing model.6.m.6.cv2.bn.bias\r\n",
      "freezing model.6.m.7.cv1.conv.weight\r\n",
      "freezing model.6.m.7.cv1.bn.weight\r\n",
      "freezing model.6.m.7.cv1.bn.bias\r\n",
      "freezing model.6.m.7.cv2.conv.weight\r\n",
      "freezing model.6.m.7.cv2.bn.weight\r\n",
      "freezing model.6.m.7.cv2.bn.bias\r\n",
      "freezing model.6.m.8.cv1.conv.weight\r\n",
      "freezing model.6.m.8.cv1.bn.weight\r\n",
      "freezing model.6.m.8.cv1.bn.bias\r\n",
      "freezing model.6.m.8.cv2.conv.weight\r\n",
      "freezing model.6.m.8.cv2.bn.weight\r\n",
      "freezing model.6.m.8.cv2.bn.bias\r\n",
      "freezing model.6.m.9.cv1.conv.weight\r\n",
      "freezing model.6.m.9.cv1.bn.weight\r\n",
      "freezing model.6.m.9.cv1.bn.bias\r\n",
      "freezing model.6.m.9.cv2.conv.weight\r\n",
      "freezing model.6.m.9.cv2.bn.weight\r\n",
      "freezing model.6.m.9.cv2.bn.bias\r\n",
      "freezing model.6.m.10.cv1.conv.weight\r\n",
      "freezing model.6.m.10.cv1.bn.weight\r\n",
      "freezing model.6.m.10.cv1.bn.bias\r\n",
      "freezing model.6.m.10.cv2.conv.weight\r\n",
      "freezing model.6.m.10.cv2.bn.weight\r\n",
      "freezing model.6.m.10.cv2.bn.bias\r\n",
      "freezing model.6.m.11.cv1.conv.weight\r\n",
      "freezing model.6.m.11.cv1.bn.weight\r\n",
      "freezing model.6.m.11.cv1.bn.bias\r\n",
      "freezing model.6.m.11.cv2.conv.weight\r\n",
      "freezing model.6.m.11.cv2.bn.weight\r\n",
      "freezing model.6.m.11.cv2.bn.bias\r\n",
      "freezing model.7.conv.weight\r\n",
      "freezing model.7.bn.weight\r\n",
      "freezing model.7.bn.bias\r\n",
      "freezing model.8.cv1.conv.weight\r\n",
      "freezing model.8.cv1.bn.weight\r\n",
      "freezing model.8.cv1.bn.bias\r\n",
      "freezing model.8.cv2.conv.weight\r\n",
      "freezing model.8.cv2.bn.weight\r\n",
      "freezing model.8.cv2.bn.bias\r\n",
      "freezing model.8.cv3.conv.weight\r\n",
      "freezing model.8.cv3.bn.weight\r\n",
      "freezing model.8.cv3.bn.bias\r\n",
      "freezing model.8.m.0.cv1.conv.weight\r\n",
      "freezing model.8.m.0.cv1.bn.weight\r\n",
      "freezing model.8.m.0.cv1.bn.bias\r\n",
      "freezing model.8.m.0.cv2.conv.weight\r\n",
      "freezing model.8.m.0.cv2.bn.weight\r\n",
      "freezing model.8.m.0.cv2.bn.bias\r\n",
      "freezing model.8.m.1.cv1.conv.weight\r\n",
      "freezing model.8.m.1.cv1.bn.weight\r\n",
      "freezing model.8.m.1.cv1.bn.bias\r\n",
      "freezing model.8.m.1.cv2.conv.weight\r\n",
      "freezing model.8.m.1.cv2.bn.weight\r\n",
      "freezing model.8.m.1.cv2.bn.bias\r\n",
      "freezing model.8.m.2.cv1.conv.weight\r\n",
      "freezing model.8.m.2.cv1.bn.weight\r\n",
      "freezing model.8.m.2.cv1.bn.bias\r\n",
      "freezing model.8.m.2.cv2.conv.weight\r\n",
      "freezing model.8.m.2.cv2.bn.weight\r\n",
      "freezing model.8.m.2.cv2.bn.bias\r\n",
      "freezing model.8.m.3.cv1.conv.weight\r\n",
      "freezing model.8.m.3.cv1.bn.weight\r\n",
      "freezing model.8.m.3.cv1.bn.bias\r\n",
      "freezing model.8.m.3.cv2.conv.weight\r\n",
      "freezing model.8.m.3.cv2.bn.weight\r\n",
      "freezing model.8.m.3.cv2.bn.bias\r\n",
      "freezing model.9.cv1.conv.weight\r\n",
      "freezing model.9.cv1.bn.weight\r\n",
      "freezing model.9.cv1.bn.bias\r\n",
      "freezing model.9.cv2.conv.weight\r\n",
      "freezing model.9.cv2.bn.weight\r\n",
      "freezing model.9.cv2.bn.bias\r\n",
      "freezing model.10.conv.weight\r\n",
      "freezing model.10.bn.weight\r\n",
      "freezing model.10.bn.bias\r\n",
      "freezing model.13.cv1.conv.weight\r\n",
      "freezing model.13.cv1.bn.weight\r\n",
      "freezing model.13.cv1.bn.bias\r\n",
      "freezing model.13.cv2.conv.weight\r\n",
      "freezing model.13.cv2.bn.weight\r\n",
      "freezing model.13.cv2.bn.bias\r\n",
      "freezing model.13.cv3.conv.weight\r\n",
      "freezing model.13.cv3.bn.weight\r\n",
      "freezing model.13.cv3.bn.bias\r\n",
      "freezing model.13.m.0.cv1.conv.weight\r\n",
      "freezing model.13.m.0.cv1.bn.weight\r\n",
      "freezing model.13.m.0.cv1.bn.bias\r\n",
      "freezing model.13.m.0.cv2.conv.weight\r\n",
      "freezing model.13.m.0.cv2.bn.weight\r\n",
      "freezing model.13.m.0.cv2.bn.bias\r\n",
      "freezing model.13.m.1.cv1.conv.weight\r\n",
      "freezing model.13.m.1.cv1.bn.weight\r\n",
      "freezing model.13.m.1.cv1.bn.bias\r\n",
      "freezing model.13.m.1.cv2.conv.weight\r\n",
      "freezing model.13.m.1.cv2.bn.weight\r\n",
      "freezing model.13.m.1.cv2.bn.bias\r\n",
      "freezing model.13.m.2.cv1.conv.weight\r\n",
      "freezing model.13.m.2.cv1.bn.weight\r\n",
      "freezing model.13.m.2.cv1.bn.bias\r\n",
      "freezing model.13.m.2.cv2.conv.weight\r\n",
      "freezing model.13.m.2.cv2.bn.weight\r\n",
      "freezing model.13.m.2.cv2.bn.bias\r\n",
      "freezing model.13.m.3.cv1.conv.weight\r\n",
      "freezing model.13.m.3.cv1.bn.weight\r\n",
      "freezing model.13.m.3.cv1.bn.bias\r\n",
      "freezing model.13.m.3.cv2.conv.weight\r\n",
      "freezing model.13.m.3.cv2.bn.weight\r\n",
      "freezing model.13.m.3.cv2.bn.bias\r\n",
      "freezing model.14.conv.weight\r\n",
      "freezing model.14.bn.weight\r\n",
      "freezing model.14.bn.bias\r\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\r\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/kaggle/input/yoga-poses-annotations/Poses/labels' images and l\u001b[0m\r\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Cache directory /kaggle/input/yoga-poses-annotations/Poses is not writeable: [Errno 30] Read-only file system: '/kaggle/input/yoga-poses-annotations/Poses/labels.cache.npy'\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/kaggle/input/yoga-poses-annotations/Poses/labels' images and lab\u001b[0m\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING: Cache directory /kaggle/input/yoga-poses-annotations/Poses is not writeable: [Errno 30] Read-only file system: '/kaggle/input/yoga-poses-annotations/Poses/labels.cache.npy'\r\n",
      "Plotting labels to Yoga_Poses/freeze_layers/labels.jpg... \r\n",
      "\r\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.46 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\r\n",
      "Image sizes 640 train, 640 val\r\n",
      "Using 1 dataloader workers\r\n",
      "Logging results to \u001b[1mYoga_Poses/freeze_layers\u001b[0m\r\n",
      "Starting training for 20 epochs...\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      0/19     13.8G   0.07746   0.02501   0.03209        27       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228    0.00144      0.358    0.00161   0.000501\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      1/19       13G   0.06073   0.02239   0.03176        28       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228    0.00164      0.939      0.147     0.0752\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      2/19       13G   0.05996   0.01885   0.03202        19       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.587       0.25      0.193      0.128\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      3/19       13G   0.05343   0.01562   0.03025        20       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.882      0.193      0.205      0.111\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      4/19       13G   0.04814   0.01407   0.02973        27       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.903      0.176      0.256       0.13\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      5/19       13G   0.04849   0.01311   0.03121        25       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.886      0.167      0.213      0.119\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      6/19     13.5G   0.04927   0.01193    0.0309        21       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.969      0.222      0.347       0.23\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      7/19     13.5G    0.0395   0.01085   0.02159        22       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.399      0.479      0.317      0.157\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      8/19     13.5G   0.04961   0.01086   0.02746        19       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.435       0.48      0.336      0.174\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "      9/19     13.8G   0.04057   0.01082   0.02266        32       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.757       0.37      0.374      0.273\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     10/19     14.3G   0.04163  0.009509   0.02118        20       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.475      0.571      0.366      0.256\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     11/19     14.3G   0.03107  0.009424    0.0172        24       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228       0.59      0.565      0.462      0.326\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     12/19     15.1G   0.03035  0.009586   0.01764        25       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228       0.54      0.588      0.427       0.29\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     13/19     15.1G   0.03305  0.008879    0.0226        23       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.521      0.578      0.418      0.323\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     14/19     15.1G   0.03019   0.00872   0.01645        20       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228        0.6      0.577      0.497      0.366\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     15/19     15.1G   0.03213  0.009265   0.01648        27       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.628       0.58      0.498      0.366\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     16/19     15.8G   0.02835  0.008411   0.01762        17       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.608      0.593      0.487      0.377\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     17/19     15.8G   0.03031  0.008433   0.01309        25       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.655      0.608      0.512      0.418\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     18/19     16.1G   0.02629  0.009173   0.01762        28       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228      0.653      0.637      0.508      0.421\r\n",
      "\r\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\r\n",
      "     19/19     16.1G   0.02208  0.008646   0.01256        25       640: 100%|███\r\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\r\n",
      "                 all        466        228       0.65      0.649      0.491      0.422\r\n",
      "\r\n",
      "20 epochs completed in 0.306 hours.\r\n",
      "Optimizer stripped from Yoga_Poses/freeze_layers/weights/last.pt, 173.2MB\r\n",
      "Optimizer stripped from Yoga_Poses/freeze_layers/weights/best.pt, 173.2MB\r\n",
      "\r\n",
      "Validating Yoga_Poses/freeze_layers/weights/best.pt...\r\n",
      "Fusing layers... \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./yolov5/train.py\", line 633, in <module>\r\n",
      "    main(opt)\r\n",
      "  File \"./yolov5/train.py\", line 529, in main\r\n",
      "    train(opt.hyp, opt, device, callbacks)\r\n",
      "  File \"./yolov5/train.py\", line 416, in train\r\n",
      "    model=attempt_load(f, device).half(),\r\n",
      "  File \"/kaggle/working/yolov5/models/experimental.py\", line 82, in attempt_load\r\n",
      "    model.append(ckpt.fuse().eval() if fuse else ckpt.eval())  # fused or un-fused model in eval mode\r\n",
      "  File \"/kaggle/working/yolov5/models/yolo.py\", line 232, in fuse\r\n",
      "    m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\r\n",
      "  File \"/kaggle/working/yolov5/utils/torch_utils.py\", line 230, in fuse_conv_and_bn\r\n",
      "    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))\r\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\r\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6468b0b710>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\r\n",
      "AttributeError: 'NoneType' object has no attribute 'python_exit_status'\r\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f6468b0b710>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\r\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1283, in _shutdown_workers\r\n",
      "AttributeError: 'NoneType' object has no attribute 'python_exit_status'\r\n"
     ]
    }
   ],
   "source": [
    "!python ./yolov5/train.py --img {SIZE}\\\n",
    "               --batch {BATCH_SIZE}\\\n",
    "               --epochs {EPOCHS}\\\n",
    "               --data ./data.yaml\\\n",
    "               --weights {MODEL}.pt\\\n",
    "               --workers {WORKERS}\\\n",
    "               --project {PROJECT}\\\n",
    "               --name freeze_layers\\\n",
    "               --exist-ok\\\n",
    "               --freeze 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d32e8",
   "metadata": {
    "papermill": {
     "duration": 1.585434,
     "end_time": "2022-08-15T06:12:45.809834",
     "exception": false,
     "start_time": "2022-08-15T06:12:44.224400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*6t1B6p8Ch1L_klZpYxLLmw.jpeg)I obtained This Image after running the model through detect.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1204.397055,
   "end_time": "2022-08-15T06:12:46.465814",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-15T05:52:42.068759",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
